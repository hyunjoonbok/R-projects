---
title: "Deep learning with TensorFlow and Keras"
author: "HyunJoon Bok"
date: "July 23, 2018"
output: html_document
---

# MNIST Example
```{r}
require(keras)
mnist <- dataset_mnist()
c(c(x_train, y_train), c(x_test, y_test)) %<-% mnist 

# Collapse each matrix to a 1-dimensional vector
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255

# One-hot encoding
a <- 10 # number of classes
y_train <- to_categorical(y_train, a)
y_test <- to_categorical(y_test, a)

dim(x_train)
dim(y_train)
```



```{r}
model <- keras_model_sequential()

model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = ncol(x_train)) %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')

summary(model)
```


```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c("accuracy")
)
```


```{r}
# batch_size = how many observations we should use per parameter update
batch_size <- 128 # Choose the power of 2 !!

# epoch = one epoch is one full pass through the training dataset
epoch <- 15

history <- model %>% fit(
  x = x_train, 
  y = y_train, 
  batch_size = batch_size,
  epochs = epoch,
  verbose = 1,
  validation_split = 0.2
)
history
plot(history)
```


```{r}
score <- model %>% evaluate(
  x = x_test,
  y = y_test,
  verbose = 0
)
score
```


Can we make a prediction and check against the actual images?
```{r}
# Predicted Class
yhat_keras_class_vec <- predict_classes(object = model, x = x_test) %>%
  as.vector()
head(yhat_keras_class_vec)
# Predicted Class Probability
yhat_keras_prob_vec  <- predict_proba(object = model, x = x_test) %>%
  as.vector()
head(yhat_keras_prob_vec)
```


## ===================================================================================================== ##

Example with CNN
Let's see if the test accuray increases!

CNN exploits the spatial relationship among the pixels.
CNN is the most popular with image data

```{r}
require(keras)
mnist <- dataset_mnist()
img_rows <- 28
img_cols <- 28

c(c(x_train, y_train), c(x_test, y_test)) %<-% mnist 

x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))

input_shape <- c(img_rows, img_cols, 1)

# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255

# One-hot encoding
a <- 10 # number of classes
y_train <- to_categorical(y_train, a)
y_test <- to_categorical(y_test, a)

dim(x_train)
dim(y_train)

```


```{r}
model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3,3),
                activation = 'relu', input_shape = input_shape) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3,3),
                strides = 2, activation = 'relu') %>% # stride: when we slide in factor, how much you want to stride by (usually some small number)
  layer_max_pooling_2d(pool_size = c(2,2)) %>% # 
  layer_flatten() %>% 
  layer_dense(units = a, activation = "softmax")

summary(model)
```
# Next steps are pretty much the same

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c("accuracy")
)

batch_size <- 32
epoch <- 15

history <- model %>% fit(
  x = x_train, 
  y = y_train, 
  batch_size = batch_size,
  epochs = epoch,
  verbose = 1,
  validation_split = 0.2
)
history

score <- model %>% evaluate(
  x = x_test,
  y = y_test,
  verbose = 0
)
score
```


## ===================================================================================================== ##

In this example, let's classify if movie reviews on IMDB are positive or negative based on the review test

```{r}
require(keras)
max_features <- 20000
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test,y_test)) %<-% imdb 
```

```{r}

```












